---
title: "STAT4255 Homework 1"
author: "Jack Bienvenue"
date: "September 18th, 2024"
format:
  html: default
  pdf: default
---

Hello Dr. Gu! This is Jack Bienvenue. I'm looking forward to a great semester of learning about Statistical Learning. This is generated with a Quarto Document in VSCode. Below are the necessary exercises for this homework assignment.


``` python
#We'll begin by importing necessary packages:
import pandas as pd
```

# Exercise 1 (ISLP Chapter 2 Exercise 2):
Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

### (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

This scenario is a regression problem. We see that we are using several variables which are continuous, quasi-continuous, or categorical such as:

- Profit
- Number of employees
- Industry 

Based on the setup of the problem, we know that **CEO salary** is the response variable we are looking to explain. 

While this problem could have applications for prediction, we are looking to make *inferences* about which factors affect CEO salary based upon our explanatory variable. Regression analysis provides many methods to evaluate the influence of predictors on a response.

Here, we have $n = 500$ for the number of firms in the dataset. We are working with $p = 3$ predictors, listed above. 

### (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

This problem would fall under the category of **classification problems**. This is made clear by the fact that our response variable is not continuous in nature by any means -- instead, the response is simply **success** or **failure**, two classes. 

Here, we are heavily vested in the idea of **prediction**. We are building a model to inform our business decision. We are assuming that there is an inference-strength connection between our predictors and response and we are using this to predict future success.

Here we are working with $n = 20$ for the number of similar products and $p = 13$ for the number of predictors considered. 

### (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

Here, we are working with another regression problem. The percentage change in an exchange rate is a continuous variable. 

Here, prediction is the certain goal of our efforts. There can be great arbitrage profits to be made by playing with exchange rates well. For market modeling, prediction is nearly always the goal.

Here we have $n = 52$, for the number of weeks in the year (assuming there are no weeks off for exchange rate calculations), and $p = 3$, the percentage changes in the US, UK, and German markets. 

# Exercise 2 (ISLP Chapter 2 Exercise 7):
The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

| Obs. | X1 | X2 | X3 | Y     |
|------|----|----|----|-------|
| 1    | 0  | 3  | 0  | Red   |
| 2    | 2  | 0  | 0  | Red   |
| 3    | 0  | 1  | 3  | Red   |
| 4    | 0  | 1  | 2  | Green |
| 5    | -1 | 0  | 1  | Green |
| 6    | 1  | 1  | 1  | Red   |

Suppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors.

### (a) Compute the Euclidean distance between each observation and the test point, $X1 = X2 = X3 = 0$.

Let's use a three-dimensional space where we model coordinates as $(X_1,X_2,X_3)$. We will obtain the following distances using 3-dimensional Euclidean distance formula: $$d = \sqrt{(X_1 - X_{\text{test}})^2 + (X_2 - X_{\text{test}})^2 + (X_3 - X_{\text{test}})^2}$$. 

Making use of this formula, we can amend our table in the following way:

| Obs. | X1 | X2 | X3 | Y     | Distance from test point|
|------|----|----|----|-------|-------------------------|
| 1    | 0  | 3  | 0  | Red   |            3            |
| 2    | 2  | 0  | 0  | Red   |            2            |
| 3    | 0  | 1  | 3  | Red   |           3.162         |
| 4    | 0  | 1  | 2  | Green |           2.236         |
| 5    | -1 | 0  | 1  | Green |           1.414         |
| 6    | 1  | 1  | 1  | Red   |           1.732         |


### (b) What is our prediction with K = 1? Why?

For $K = 1$ nearest neighbors, our test point $(0,0,0)$ is closest to the point for observation 5, $(-1,0,1)$, and since this is the only neighbor we are taking into account, and its response value is $\textbf{Green}$, we will take our response value for test point $(0,0,0)$ to be $\textbf{Green}$.

### (c) What is our prediction with K = 3? Why?

For $K = 3$ nearest neighbors, test point $(0,0,0)$ has nearest neighbors:

| Obs. | X1 | X2 | X3 | Y     | Distance from test point|
|------|----|----|----|-------|-------------------------|
| 5    | -1 | 0  | 1  | Green |           1.414         |
| 6    | 1  | 1  | 1  | Red   |           1.732         |
| 2    | 2  | 0  | 0  | Red   |            2            |

Since we find that the majority of our neighbors have response $Y$ value as $\textbf{Red}$, we will identify our test point as $\textbf{Red}$.


### (d) If the Bayes decision boundary in this problem is highly non- linear, then would we expect the best value for K to be large or small? Why?

For highly non-linear Bayes decision boundary, we would select a smaller $\textbf{K}$ for the modeling of our scenario given a choice. How come? The addition of nearest neighbors has something a smoothing effect on the decision boundary. Smaller $\textbf{K}$ values will allow for smaller nuances in the decision boundary to be present, while these might be smoothed out with a larger $\textbf{K}$. This is important for non-linear data because very high relative $\textbf{K}$ values could appear to fit a decision boundary to data that looks more linear, or simpler in general, potenially not capturing the nature of the data. 

# Exercise 3 (ISLP Chapter 2 Exercise 8):

This exercise relates to the College data set, which can be found in the file College.csv on the book website. It contains a number of variables for 777 different universities and colleges in the US. The variables are:

- **Private**: Public/private indicator
- **Apps**: Number of applications received
- **Accept**: Number of applicants accepted
- **Enroll**: Number of new students enrolled
- **Top10perc**: New students from top 10% of high school class
- **Top25perc**: New students from top 25% of high school class
- **F.Undergrad**: Number of full-time undergraduates
- **P.Undergrad**: Number of part-time undergraduates
- **Outstate**: Out-of-state tuition
- **Room.Board**: Room and board costs
- **Books**: Estimated book costs
- **Personal**: Estimated personal spending
- **PhD**: Percent of faculty with Ph.D.s
- **Terminal**: Percent of faculty with terminal degree
- **S.F.Ratio**: Student/faculty ratio
- **perc.alumni**: Percent of alumni who donate
- **Expend**: Instructional expenditure per student
- **Grad.Rate**: Graduation rate

Before reading the data into Python, it can be viewed in Excel or a text editor.

### (a) Use the pd.read_csv() function to read the data into Python. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.

``` {python}
import pandas as pd
college_df = pd.read_csv('data/College.csv')
```

### (b) Look at the data used in the notebook by creating and running a new cell with just the code college in it. 
You should notice that the first column is just the name of each university in a column named something like Unnamed: 0. We donâ€™t really want pandas to treat this as data. However, it may be handy to have these names for later. 

``` {python}
college_df
```

Try the following commands and similarly look at the resulting data frames:

``` {python}
college2 = pd.read_csv('data/College.csv', index_col=0) 

college3 = college_df.rename({'Unnamed: 0': 'College'}, axis=1) 
college3 = college3.set_index('College')

college2
college3
```

This has used the first column in the file as an index for the data frame. This means that pandas has given each row a name corresponding to the appropriate university. Now you should see that the first data column is Private. Note that the names of the colleges appear on the left of the table. We also introduced a new python object above: a dictionary, which is specified by (key, value) pairs. Keep your modified version of the data with the following:

``` {python}
college = college3
```

### (c) Use the describe() method of to produce a numerical summary of the variables in the data set.

``` {python}
college.describe()
```

### (d) Use the pd.plotting.scatter_matrix() function to produce a scatterplot matrix of the first columns [Top10perc, Apps, Enroll]. Recall that you can reference a list C of columns of a data frame A using A[C].

``` {python}
import matplotlib.pyplot as plt

plot_subset = college[['Top10perc', 'Apps', 'Enroll']]
pd.plotting.scatter_matrix(plot_subset, diagonal = 'kde') # Here, I'm adding the diagonal argument to get a smooth line plot instead of histogram
plt.suptitle('Students in Top 10% of Class, Applications, and Enrollment', fontsize=16)
plt.show()
```

### (e) Use the boxplot() method of college to produce side-by-side boxplots of Outstate versus Private.

``` {python}
q3edf = college[['Outstate', 'Private']]
q3edf.boxplot(column = 'Outstate', by='Private', figsize = (8,6))
plt.title("College Out of State Tuition vs. Privacy Status")
plt.ylabel('Out of state Tuition ($)')
plt.xlabel('Private Status')

```

### (f) Create a new qualitative variable, called Elite, by binning the Top10perc variable into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.
Use the value_counts() method of college['Elite'] to see how many elite universities there are. Finally, use the boxplot() method again to produce side-by-side boxplots of Outstate versus Elite.
   

``` {python} 
#Generate 'Elite'
college['Elite'] = pd.cut(college['Top10perc'], bins=[0,50,100],
labels=['No', 'Yes'])

elite_counts = college['Elite'].value_counts()
print('There are', elite_counts['Yes'], 'elite colleges in the dataset.')
```

``` {python}
#Generate boxplots
q3edf = college[['Outstate', 'Elite']]
q3edf.boxplot(column = 'Outstate', by='Elite', figsize = (8,6))
plt.title("College Out of State Tuition vs. Elite Identity")
plt.ylabel('Out of state Tuition ($)')
plt.xlabel('Elite Indicator')
```

### (g) Use the plot.hist() method of college to produce some his- tograms with differing numbers of bins for a few of the quanti- tative variables. 
The command plt.subplots(2, 2) may be useful: it will divide the plot window into four regions so that four plots can be made simultaneously. By changing the arguments you can divide the screen up in other combinations.

``` {python}
fig, axs = plt.subplots(2, 2, figsize=(8, 6))
# Histogram for 'Elite'
axs[0, 0].hist(college['Enroll'], bins=20, color='skyblue', edgecolor='black')
axs[0, 0].set_title('Histogram of Enrollment')
axs[0, 0].set_xlabel('Enrollment Count')
axs[0, 0].set_ylabel('Count')

# Histogram for 'Outstate'
axs[0, 1].hist(college['Outstate'], bins=20, color='blue', edgecolor='black')
axs[0, 1].set_title('Histogram of Out of State Tuition')
axs[0, 1].set_xlabel('Outstate Tuition')
axs[0, 1].set_ylabel('Count')

# Histogram for 'Apps'
axs[1, 0].hist(college['Apps'], bins=20, color='lightgreen', edgecolor='black')
axs[1, 0].set_title('Histogram of Applications')
axs[1, 0].set_xlabel('Number of Applications')
axs[1, 0].set_ylabel('Count')

# Histogram for 'Room.Board'
axs[1, 1].hist(college['Room.Board'], bins=20, color='orange', edgecolor='black')
axs[1, 1].set_title('Histogram of Room and Board Costs')
axs[1, 1].set_xlabel('Room and Board Cost')
axs[1, 1].set_ylabel('Count')

# Adjust layout
plt.tight_layout()
plt.show()
```


### (h) Continue exploring the data, and provide a brief summary of what you discover.

From digging deeper into this dataset we can see the presence of some interesting effects.

- It seems that many colleges have relatively low application and enrollment numbers
    - This could suggest that the college environment is made of many large colleges but many, many more small ones
- Costs for college have some skewing to the right
    - This means that while there is a concentration of values for the costs associated with attending college, there are some colleges whose cost is significantly higher than the average

# Exercise 4 (ISLP Chapter 2 Exercise 8):

### Question 1
The volume of the containing box in p-dimensional space can be represented simply in terms of length l as $l^p$.

### Question 2
This is a little trickier than the previous problem. The volume of a unit "ball" in $p$ dimensions is: $$V_p = \frac{\pi^{p/2}}{\Gamma\left(\frac{p}{2} + 1\right)}$$

where $V_p$ is the volume (or analog), and the Gamma function in the denominator is an important tool to determine coefficients. The definition and origin of the Gamma Function is beyond the scope of this assignment. 

### Question 3
The ratio $\alpha(p)$ would take the form:
$$\frac{l^p}{\frac{\pi^{p/2}}{\Gamma\left(\frac{p}{2} + 1\right)}}$$

We can make a function to calculate this for varying p:

``` {python}
from scipy.special import gamma
import numpy as np

def volume_of_unit_sphere(p):
    return (np.pi ** (p / 2)) / gamma((p / 2) + 1)

def cube_sphere_ratio(cube_side_length, dimension):
    ratio = ((cube_side_length)**dimension)/(volume_of_unit_sphere(dimension))

    return ratio
```

### Question 4
Now, let's visualize what's going on. 
``` {python}
dimension_df = pd.DataFrame({
    'Dimension': range(1,21),
    'Ratio': np.nan

})
for p in range(1,21):
    ratio_outcome = cube_sphere_ratio(1, p)
    dimension_df.loc[p-1, 'Ratio'] = ratio_outcome

print(dimension_df)    
```

``` {python}
plt.figure(figsize=(10, 6))
plt.plot(dimension_df['Dimension'], dimension_df['Ratio'], marker='o')
plt.title('Cube/Sphere Ratio vs Dimension')
plt.xlabel('Dimension')
plt.ylabel('"Cube Volume" to "Sphere Volume" Ratio')
plt.xticks(dimension_df['Dimension'])  
plt.grid()
plt.show()
```

### Question 5

After modeling the situation, it does appear that the so-called Curse of Dimensionality is in effect. Here, it is clear that as the dimension is expanded, the volume of cube shapes become much larger relative to the sphere shape. This means that given randomly distributed data, we will see a behavior of less and less data falling within the inscribed "sphere" as the dimension rises. 
